# Paper

**Reflective Manifold: Geometric Structure in Large Language Model Self-Reflection**

## File

- `reflective-manifold.pdf` - Final compiled paper (3.5 MB)

## Abstract

This paper presents an empirical investigation into the geometric structure of large language model (LLM) self-reflection. Through systematic experiments across five model families (Claude, GPT, Gemini, Grok, DeepSeek), we discover that recursive reflection induces consistent, measurable structure in embedding space—a "reflective manifold" with stable attractor basins and provider-invariant topology.

## Key Findings

| Finding | Evidence |
|---------|----------|
| Style dominates provider identity | R² = 17.2% (style) vs 9.9% (provider) |
| Attractor basins form by loop 3 | Convergence radius stabilizes |
| Topology is provider-invariant | PERMANOVA p < 0.001 |

## Citation

```bibtex
@article{brown2025reflective,
  title={Reflective Manifold: Geometric Structure in Large Language Model Self-Reflection},
  author={Brown, Lee and Brown, Lucas},
  year={2025},
  url={https://github.com/geeks-accelerator/reflection-manifold-data}
}
```

---

*See [arxiv/](../arxiv/) for submission materials.*
